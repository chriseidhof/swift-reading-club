http://okmij.org/ftp/tagless-final/sharing/sharing.pdf

# Chris

Here's my annotated source code of the paper:

The sharing paper was really interesting and readable. Most of it is reproducible in Swift. Ideally, we'd like to write the Haskell type-class like this:

```swift
// protocol Expr {
//     static func constant(_: Int) -> Self<Int>
//     static func variable(_: String) -> Self<Int>
//     static func add(_: Self<Int>, _: Self<Int>) -> Self<Int>
// }
```

However, because we don't have higher-kinded types, we're forced to write it like this:

```swift
protocol Expr {
    static func constant(_: Int) -> Self
    static func variable(_: String) -> Self
    static func add(_: Self, _: Self) -> Self
}
```

The "normal" enum 
 
```swift
indirect enum Exp {
    case _const(Int)
    case _variable(String)
    case _add(Exp, Exp)
}
```
 
We can make it conform to `Expr`

```swift
extension Exp: Expr {
    static func constant(_ x: Int) -> Exp {
        return _const(x)
    }
    
    static func variable(_ x: String) -> Exp {
        return _variable(x)
    }
    
    static func add(_ l: Exp, _ r: Exp) -> Exp {
        return _add(l, r)
    }
}
```

## Sample Expressions

And here are some sample expressions. Note that they have a polymorphic return type! We can't declare polymorphic variables (also because `Expr` has an associated type), but we can declare these values:
 
```swift
func exp_b<E: Expr>() -> E {
    let exp_a: E = .add(.constant(10), .variable("i1"))
    return .add(exp_a, .variable("i2"))
}
func mul<E: Expr>(_ x: Int, _ y: E) -> E {
    switch x {
    case 0: return .constant(0)
    case 1: return y
    case let n where n % 2 == 0:
        return mul(n/2, .add(y,y))
    case let n:
        return E.add(y, mul(n-1, y))
    }
}

func exp_mul4<E: Expr>() -> E {
    return mul(4, .variable("i1"))
}

dump(exp_b() as Exp)
```

## Evaluation

Now let's port the evaluator. First, we define the environment mapping variable names to values:

```swift
typealias REnv = [String:Int]
```

Next, we define a reader. This is really just "dependency injection", but in a functional way.

```swift
struct R { let unR: (REnv) -> Int }
extension R: Expr {
    static func constant(_ x: Int) -> R {
        return R { _ in x }
    }
    static func variable(_ n: String) -> R {
        return R { env in env[n]! }
    }
    static func add(_ l: R, _ r: R) -> R {
        return R { l.unR($0) + r.unR($0) }
    }
}

(exp_mul4() as R).unR(["i1":5])
```

## Hash-consing

Next up, we port the hashconsing/DAG building. We'll start by defining the node type. Note that it is not recursive, but instead of recursing, it has NodeIDs. (Maybe we could build this representation with a generic programming library such as [Regular](https://hackage.haskell.org/package/regular), not sure?)


```swift
typealias NodeID = Int
enum Node {
    case nconst(Int)
    case nvar(String)
    case nadd(NodeID,NodeID)
}
extension Node: Equatable {
    static func ==(lhs: Node, rhs: Node) -> Bool {
        switch (lhs, rhs) {
        case (.nconst(let l), .nconst(let r)): return l == r
        case (.nvar(let l), .nvar(let r)): return l == r
        case (.nadd(let l), .nadd(let r)): return l == r
        default: return false
        }
    }
}
```

Here's a simple Bimap implementation like in the paper. It maps ints (`NodeID`s) to values (`Node`s).

```swift
struct Bimap<A> where A: Equatable {
    private var storage: [A] = []
    
    subscript(val value: A) -> Int? {
        return storage.index(where: { $0 == value})
    }
    subscript(key key: Int) -> A {
        return storage[key]
    }
    
    mutating func insert(_ value: A) -> Int {
        storage.append(value)
        return storage.count - 1
    }
}
```

This is the state monad in Swift. I'm not sure if there's a more swifty way of doing this, but it turns out it's not that horrible. A type `State<S,R>` describes a computation with state `S` which returns an `R`. In Swift, we can use `inout` to write things in a nicer way.
 
```swift
struct State<S,R> {
    let modify: (inout S) -> R
    func run(_ s: S) -> (R, S) {
        var x = s
        let result = modify(&x)
        return (result, x)
    }
    init(modify: @escaping (inout S) -> R) {
        self.modify = modify
    }
}
```

A DAG is simply a Bimap of Nodes.

```swift
typealias DAG = Bimap<Node>
```

Our node numbering is wrapped up in `N`
 
```swift
struct N {
    let unN: State<DAG, NodeID>
}

extension N {
    var run: (NodeID, DAG) {
        return unN.run(DAG())
    }
}

```
Hashconsing is much shorter than in the original paper:
 
```swift
extension Node {
    var hashcons: State<DAG,NodeID> {
        return State { (s: inout DAG) in
            s[val: self] ?? s.insert(self)
        }
    }
}
```

All that's left is making `N` conform to `Expr`:
 
```swift
extension N: Expr {
    static func constant(_ x: Int) -> N {
        return N(unN: Node.nconst(x).hashcons)
    }
    static func variable(_ s: String) -> N {
        return N(unN: Node.nvar(s).hashcons)
    }
    static func add(_ e1: N, _ e2: N) -> N {
        return N(unN: State { (state: inout DAG) in
            let h1 = e1.unN.modify(&state)
            let h2 = e2.unN.modify(&state)
            return Node.nadd(h1, h2).hashcons.modify(&state)
        })
    }
}

dump((exp_mul4() as N).run)
```

# bkase

I was particularly fascinated by the `sklansky` function even though it was not a big part of the paper. `sklansky` blew my mind. Let's just focus on that.

We need to start with Swift's `reduce` (which is a left-fold) in FP-speak. I'm going to simplify it slightly so you can't change the return type (not as useful, but easier to motivate sklansky)

```swift
extension Array {
  // implemented recursively and super inneficiently, but gets the point across
  func reduce(baseCase: Element, combine: (Element, Element) -> Element) -> Element {
    guard let head = self.first else {
      return baseCase
    }

    return dropFirst().reduce(baseCase: combine(head, baseCase), combine: combine)
  }
}
```

We can use reduce (or fold or cata) to destroy a sequence (here an Array) down to one element. For example `[1,2,3].reduce(0, +)` returns `6`.
Sometimes though, we actually want the intermediate steps along the way. This is traditionally called a scan. Here is a left-scan on Array (also hilariously inneficient)

```swift
extension Array {
  // implemented recursively and super inneficiently, but gets the point across
  func scan(baseCase: Element, combine: (Element, Element) -> Element) -> [Element] {
    func helper(running: [Element], combine: (Element, Element) -> Element) -> [Element]  {
      guard let head = self.first else {
        return running
      }
      return dropFirst().helper(running: running + combine(head, baseCase), combine: combine)
    }

    return helper(running: [baseCase], combine: combine)
  }
}
```

This is very similar to reduce, but instead of forgetting our intermediate results we aggregate them in `running`. Now we can get the sub-states of our folds which is really useful in some cases!

Let's switch gears. We live in a world of many cores. This is getting more and more true and we're Moore's law is stalling and we're starting to build general compute programs for the GPU. The reduce and scan implementations make no assumptions about how the combine works, and so we can't take advantage of our many cores.
But, if I know that my operation is associative, then I can safely reorder my parenthesization and I won't change the end result of the computation. If we group pairs of functions then we can get more performance.
I'll omit the implementation of a parallel-friendly reduce, but it's not too bad, and just jump to sklansky.

```swift
extension Array {
    func sklansky(_ f: (Element, Element) -> Element) -> [Element] {
        if self.count == 0 {
            return self
        }
        if self.count == 1 {
            return self
        }

        let (left, right) = (Array(self[0..<self.count/2]), Array(self[self.count/2..<self.count]))
        let left_ = left.sklansky(f)
        let right_ = right.sklansky(f)
        return left_ + right_.map{ f(left_.last!, $0) }
    }
}
```

Base cases are at 0 and 1 elements, then we the left and the right (at this point since we know we started with at least two elements, we have at least one element in the left and the right halves. We then recurse on the left and on the right.

Finally we do something: We concat the left (after recursion) with (for each thing on the right) combined with the latest thing on the left.

Not super straightforward -- it's illuminating to look at what happens to a pretty-printed expression (from the paper):

```swift
[1,2,3,4].map{ .var("v\($0)") }.sklansky(.add) =
  ["v1","(v1+v2)","((v1+v2)+v3)","((v1+v2)+(v3+v4))"]
```

Woah! This is a parallel-efficient scan! Mind blowing. It's certainly is not mind-blowing that this is possible, I just haven't thought or ever read about it before.
